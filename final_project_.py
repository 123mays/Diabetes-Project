# -*- coding: utf-8 -*-
"""Final Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jOI_dtO0GbPEu9m0g_Yd0thci7rt7dE

Project:Predicting Diabetes Among Women in the Pima Indian Population Using Deep Learning



Name: Mays Neiroukh

Date: 3/13/2024

# Abstract
In this project, I explore the application of neural networks to predict diabetes among women in the Pima Indian population. Driven by the global rise in diabetes and the critical role of early detection, I adopt a deep learning approach to analyze clinical data for predicting the presence of diabetes. The results demonstrate decent accuracy, showcasing the potential of my neural network model.

# Introduction
Diabetes is a widespread chronic disease with significant health consequences. The International Diabetes Federation (IDF) reports that 382 million people worldwide are living with diabetes. In recent years, the impact of diabetes has increased dramatically, making it a global threat. Currently, diabetes is consistently ranked as a leading cause of death. Given the number of people affected by the disease, early prediction and diagnosis are essential for effective management and treatment. In this project, I aim to leverage neural networks to predict diabetes using clinical data from the National Institute of Diabetes and Digestive and Kidney Diseases, focusing specifically on the Pima Indian population. By analyzing variables such as glucose levels, BMI, age, and others, I develop a model capable of predicting the presence of diabetes with decent accuracy.

# Related Work

I have explored several studies that utilize machine learning for diabetes prediction, employing algorithms that range from logistic regression to decision trees. Shanthi et al. (2015) demonstrated the effectiveness of SVMs in diabetes prediction, while Kavakiotis et al. (2017) highlighted the use of various machine learning algorithms for analyzing diabetes data. Furthermore, Naz H and Ahuja S (2020) explored a deep learning approach for diabetes prediction using the PIMA Indian dataset, achieving promising results that underscore the potential of advanced neural networks in medical diagnostics. My approach builds upon these foundations, emphasizing the capability of neural networks to handle complex relationships between clinical measurements and their application in accurately predicting the presence of diabetes. The study by Naz H and Ahuja S, in particular, provides valuable insights into the effectiveness of deep learning models, which inform the methodology and expectations of my project.

# Dataset
The dataset utilized in this neural network is from the National Institute of Diabetes and Digestive and Kidney Diseases and is specifically curated to predict the onset of diabetes within the Pima Indian population. This dataset is an example of a binary classification problem, where the outcome variable is binary, indicating the presence (1) or absence (0) of diabetes.

The dataset is composed of several medical predictor variables (features) such as the number of pregnancies, plasma glucose concentration, diastolic blood pressure, triceps skinfold thickness, 2-hour serum insulin, body mass index, diabetes pedigree function, and age. These features were used as inputs for the neural network, providing a multi-dimensional representation of each patient's medical profile.

In terms of preprocessing, the dataset underwent several steps to ensure optimal model performance. This included splitting the data into training and testing sets, with 80% of the data allocated for training and the remaining 20% for testing. Additionally, certain features underwent normalization and standardization to bring them to a similar scale, thereby avoiding potential bias toward variables with larger magnitudes. No data augmentation techniques were applied, as this dataset consists of structured data rather than image or text data which commonly require augmentation.

The number of classes for the outcome variable is two, as it is a binary classification task (diabetes or no diabetes). The total number of samples included in the dataset was 768 datapoints, which is standard for medical datasets of this type.

# Methods
 My model employs a multi-layer perceptron neural network with two hidden layers. I utilize ReLU activation functions in the hidden layers and a sigmoid output for binary classification. Based on various studies, the sigmoid function has proven to be the best activation function for the output. I chose the Adam optimizer for its efficiency in handling sparse gradients and adaptive learning rates.

The data for this project includes 768 data points, which is relatively small. Therefore, I concentrated on data preprocessing as a first step in developing my model. This approach helped me avoid bias in the process and ensure that the data is balanced in the process. The steps below shows the approach I took to accomplish my goal.
"""

# Imports
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import os
import sys
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive/', force_remount= True)

FOLDERNAME ='/content/drive/MyDrive/cs320/project '
assert FOLDERNAME is not None, "[!] Enter the foldername."

# Change the current working directory to FOLDERNAME

sys.path.insert(1, '/content/drive/MyDrive/{}'.format(FOLDERNAME))
sys.path.append(FOLDERNAME)

os.chdir(FOLDERNAME)
current_working_directory = os.getcwd()
print(f"Current Working Directory: {current_working_directory}")

"""#  1. Data Exploration

This process included exploring each features' ranges and the ratio between diabetes and  non-diabetes cases.

"""

diabetes= pd.read_csv('/content/drive/MyDrive/cs320/project /diabetes.csv')

diabetes.head()
diabetes.info()

# Visualize missing data using a heatmap to easily identify missing values

sns.heatmap(diabetes.isnull(),cmap = 'magma',cbar = False);

"""**No null vlaues are present in the data as the heatmap is all black. Therefore all datapoints could be used!**

"""

# Description of the data
diabetes.describe().T

colors = ['#446BAD','#42f5b9']
diabetes_cases = diabetes[diabetes['Outcome'] == 1].describe().T
non_diabetes_cases = diabetes[diabetes['Outcome'] == 0].describe().T

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(diabetes_cases[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('Diabetes');

plt.subplot(1,2,2)
sns.heatmap(non_diabetes_cases[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Non-Diabetes');

fig.tight_layout(pad = -1)

# This code vizualizes the ratio berwen diabetes and non-diabetes cases.

l = list(diabetes['Outcome'].value_counts())
circle = [l[0] / sum(l) * 100, l[1] / sum(l) * 100]

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))

# First subplot for the pie chart
axs[0].pie(circle, labels=['Non-Diabetes Cases', 'Diabetes Cases'], autopct='%1.1f%%', startangle=90, explode=(0.1, 0), colors=colors,
           wedgeprops={'edgecolor': 'black', 'linewidth': 1, 'antialiased': True})
axs[0].set_title('Diabetes - Non-Diabetes Case %')

# Second subplot for the countplot
axs[1] = sns.countplot(x='Outcome', data=diabetes, palette=colors, edgecolor='black')
for rect in axs[1].patches:
    axs[1].text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, f'{rect.get_height()}', ha='center', fontsize=11)
axs[1].set_xticklabels(['Non-Diabetes Cases', 'Diabetes Cases'])
axs[1].set_title('Number of Diabetes - Non-Diabetes Cases')

plt.show()

"""**Looking at the data, we can see that it's unbalanced. There are more datapoints for non-diabetes patient cases than diabetes cases.
1:2 ratio for diabetes: non-diabetes. This will make the data biased for non-daibetes. Therefore, it is necessary to balance data in the next steps.**

**Classification of features**

This step was helpful to ensure that the outcome is the only categorical variable, and the other variables are numerical.
"""

col = list(diabetes.columns)
categorical_features = []
numerical_features = []
for i in col:
    if len(diabetes[i].unique()) > 6:
        numerical_features.append(i)
    else:
        categorical_features.append(i)

print('Categorical Features :',*categorical_features)
print('Numerical Features :',*numerical_features)

# This code looks at the distribution of each feature in the input.
import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = [feature for feature in diabetes.columns if len(diabetes[feature].unique()) > 6][:8]

# Set up the matplotlib figure to accommodate 8 plots for the selected features before scaling
fig, axes = plt.subplots(nrows=8, ncols=1, figsize=(10, 40))

for i, feature in enumerate(numerical_features):
    # Plotting using histplot for each feature
    sns.histplot(diabetes[feature], ax=axes[i], color='blue', kde=True)
    axes[i].set_title(f'Distribution: {feature}')

# Adjust the layout and show the plot
plt.tight_layout()
plt.show()

"""Looking at the distribution of each feature above, we can see that Pregnancies, Insulin, DiabetesPedigreeFunction and Age features are skewed, which means they require to be normalized. BloodPressure, Skin Thickness, Glucose & BMI highlight a bidmodal data distribution, so they can be standarized.

# 2. Data Scaling
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler
import pandas as pd

# Initialize the scalers
mms = MinMaxScaler() # Normalization
ss = StandardScaler() # Standardization

# Copy the original DataFrame
df1 = diabetes.copy(deep=True)

# Scaling the columns using minmax scaler
df1['Pregnancies'] = mms.fit_transform(df1[['Pregnancies']])
df1['Age'] = mms.fit_transform(df1[['Age']])
df1['Insulin'] = mms.fit_transform(df1[['Insulin']])
df1['DiabetesPedigreeFunction'] = mms.fit_transform(df1[['DiabetesPedigreeFunction']])

# Scaling the columns using standard scaler
df1['BloodPressure'] = ss.fit_transform(df1[['BloodPressure']])
df1['SkinThickness'] = ss.fit_transform(df1[['SkinThickness']])
df1['Glucose'] = ss.fit_transform(df1[['Glucose']])
df1['BMI'] = ss.fit_transform(df1[['BMI']])

"""**First attempt to normalize the features:  Pregnancies, Insulin, DiabetesPedigreeFunction and Age**"""

import matplotlib.pyplot as plt
import seaborn as sns


numerical_features = [feature for feature in diabetes.columns if len(diabetes[feature].unique()) > 6][:8]

# Set up the matplotlib figure to accommodate 8 pairs of plots (before and after scaling)
fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(15, 40))

for i, feature in enumerate(numerical_features):
    # Plot before scaling using histplot for each feature
    sns.histplot(diabetes[feature], ax=axes[i, 0], color='blue', kde=True, label='Original')
    axes[i, 0].set_title(f'Distribution Before Scaling: {feature}')
    axes[i, 0].legend()

    # Plot after scaling using histplot for the same feature
    sns.histplot(df1[feature], ax=axes[i, 1], color='orange', kde=True, label='Scaled')
    axes[i, 1].set_title(f'Distribution After Scaling: {feature}')
    axes[i, 1].legend()

# Adjust the layout and show the plot
plt.tight_layout()
plt.show()

"""Looking at the plots before and after scaling, it seems like the skewed features haven't been normalized fully even after using the minmaxscaler.

**2nd attempt to normalize the features:  Pregnancies, Insulin, DiabetesPedigreeFunction and Age**
"""

from sklearn.preprocessing import RobustScaler

# Initialize the RobustScaler
rs = RobustScaler()

# Copy the original DataFrame
df3 = diabetes.copy(deep=True)

# Scaling the columns with RobustScaler
df3['Pregnancies'] = rs.fit_transform(df3[['Pregnancies']])
df3['Age'] = rs.fit_transform(df3[['Age']])
df3['Insulin'] = rs.fit_transform(df3[['Insulin']])
df3['DiabetesPedigreeFunction'] = rs.fit_transform(df3[['DiabetesPedigreeFunction']])

import matplotlib.pyplot as plt
import seaborn as sns


numerical_features = [feature for feature in diabetes.columns if len(diabetes[feature].unique()) > 6][:8]

# Set up the matplotlib figure to accommodate 8 pairs of plots (before and after scaling)
fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(15, 40))

for i, feature in enumerate(numerical_features):
    # Plot before scaling using histplot for each feature
    sns.histplot(diabetes[feature], ax=axes[i, 0], color='blue', kde=True, label='Original')
    axes[i, 0].set_title(f'Distribution Before Scaling: {feature}')
    axes[i, 0].legend()

    # Plot after scaling using histplot for the same feature
    sns.histplot(df3[feature], ax=axes[i, 1], color='orange', kde=True, label='Scaled')
    axes[i, 1].set_title(f'Distribution After Scaling: {feature}')
    axes[i, 1].legend()

# Adjust the layout and show the plot
plt.tight_layout()
plt.show()

"""Unfortunately, the Robust scaler didn't help with normalizing the features:  pregnancies, insulin, diabetesPedigreeFunction and age. They still look skewed.

**3rd attempt to the features:  Pregnancies, Insulin, DiabetesPedigreeFunction and Age**
"""

from sklearn.preprocessing import QuantileTransformer

# Initialize the QuantileTransformer
qt = QuantileTransformer(output_distribution='normal', random_state=0)

# Copy the original DataFrame
df4 = diabetes.copy(deep=True)

# Transforming the columns with QuantileTransformer
df4['Pregnancies'] = qt.fit_transform(df4[['Pregnancies']])
df4['Age'] = qt.fit_transform(df4[['Age']])
df4['Insulin'] = qt.fit_transform(df4[['Insulin']])
df4['DiabetesPedigreeFunction'] = qt.fit_transform(df4[['DiabetesPedigreeFunction']])

import matplotlib.pyplot as plt
import seaborn as sns


# Ensure numerical_features contains 8 desired features
numerical_features = [feature for feature in diabetes.columns if len(diabetes[feature].unique()) > 6][:8]

# Set up the matplotlib figure to accommodate 8 pairs of plots (before and after scaling)
fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(15, 40))

for i, feature in enumerate(numerical_features):
    # Plot before scaling using histplot for each feature
    sns.histplot(diabetes[feature], ax=axes[i, 0], color='blue', kde=True, label='Original')
    axes[i, 0].set_title(f'Distribution Before Scaling: {feature}')
    axes[i, 0].legend()

    # Plot after scaling using histplot for the same feature
    sns.histplot(df4[feature], ax=axes[i, 1], color='orange', kde=True, label='Scaled')
    axes[i, 1].set_title(f'Distribution After Scaling: {feature}')
    axes[i, 1].legend()

# Adjust the layout and show the plot
plt.tight_layout()
plt.show()

"""Using the QuantileTransformer seems to be the best solution to normalize the features as it it normally distributed my features. However, the distribution of insulin seems to be off as it is hard to normalize. Therefore, I will use minmax scaler for it instead.

"""

# Final version of the dataset to be used
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Initialize the StandardScaler
ss = StandardScaler() # Standardization
mms = MinMaxScaler() # Normalization


# Initialize the QuantileTransformer
qt = QuantileTransformer(output_distribution='normal', random_state=0)

# Copy the original DataFrame
df5= diabetes.copy(deep=True)

df5['BloodPressure'] = ss.fit_transform(df5[['BloodPressure']])
df5['SkinThickness'] = ss.fit_transform(df5[['SkinThickness']])
df5['Glucose'] = ss.fit_transform(df5[['Glucose']])
df5['BMI'] = ss.fit_transform(df5[['BMI']])

df1['Insulin'] = mms.fit_transform(df1[['Insulin']])

# Transforming the columns with QuantileTransformer
df4['Pregnancies'] = qt.fit_transform(df5[['Pregnancies']])
df4['Age'] = qt.fit_transform(df5[['Age']])
df4['DiabetesPedigreeFunction'] = qt.fit_transform(df5[['DiabetesPedigreeFunction']])

"""**df5 is the final version of the data normalized.**

# Correlation
This step is meant to exlpore the correlation between the features.
"""

corr = df5.corrwith(df5['Outcome']).sort_values(ascending = False).to_frame()
corr.columns = ['Correlations']
plt.subplots(figsize = (5,5))
sns.heatmap(corr,annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black');
plt.title('Correlation with respect to Outcome');

"""We can see that glucose is highy correlted with the outcome, while 'Skinthickness' and 'BloodPressure' is barely correlated with having diabetes."""

plt.figure(figsize = (20,5))
sns.heatmap(df5.corr(),cmap = colors,annot = True);

"""# 3. ANOVA Test

To confirm which features should be dropped from the dataset, I am using ANOVA test that will show me the importance of each feature.
"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

features = df5.loc[:,numerical_features]
target = df5.loc[:,categorical_features]

best_features = SelectKBest(score_func = f_classif,k = 'all')
fit = best_features.fit(features,target)

featureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['ANOVA Score'])

plt.subplots(figsize = (5,5))
sns.heatmap(featureScores.sort_values(ascending = False,by = 'ANOVA Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');
plt.title('Selection of Numerical Features');

"""Looking at the ANOVA test scores, we can confirm that 'BloodPressure' is not important for this model since it seems to be unrelated to having diabetes.

I would like to note that I initally dropped the both 'BloodPressure' and 'SkinThickness', but I decided to keep 'SkinThickness' to give my model more data to work with. However, it didn't make a noticable difference in the results/ accuracy.  
"""

# Dropping the column 'BloodPressure' from my dataframe.
df5.drop(columns = ['BloodPressure'],inplace = True)
f1 = df4.iloc[:,:6].values
t1 = df4.iloc[:,6].values

df2 = df5.copy(deep = True)
df2.head()

"""The heatmap down below shows that all features have correlation with the outcome at different levels."""

# heatmap to have the correlations
plt.figure(figsize = (20,5))
sns.heatmap(df2.corr(),cmap = colors,annot = True);

"""# 4. Balance Data

I will balance data to avoid bais through oversampling the diabetes case data using SMOTE. This step is crucial due the 1:2 ratio that is apparent between the diabetes and non-diabetes cases.
"""

import imblearn
from collections import Counter
from imblearn.over_sampling import SMOTE


# Initialize SMOTE
over = SMOTE()

f2 = df2.iloc[:, :7].values  # Features
t2 = df2.iloc[:, 7].values   # Target

# Perform resampling
f2_resampled, t2_resampled = over.fit_resample(f2, t2)

# Check the distribution after resampling
print(Counter(t2_resampled))

# Correctly create df1_resampled DataFrame
# Ensure to include all necessary columns from df1 for features
df2_resampled = pd.DataFrame(f2_resampled, columns=df2.columns[:7])

# Adjust the target column name as per your df1 structure. Here, assuming the target is the 7th column.
df2_resampled[df2.columns[7]] = t2_resampled

"""Data is balance with 500 datapoints for each case and 1000 datapoints in total.

# Building the model


Model 1 and Model 2 are sequential neural networks designed for binary classification tasks. Both employ a series of densely connected layers with ReLU activations, known for their effectiveness in adding non-linearities to the model, which helps in learning complex patterns. Model 1 has an input layer of 40 neurons, followed by a dropout layer at a rate of 50% to prevent overfitting by randomly deactivating neurons during training, and then a subsequent layer of 32 neurons before reaching the output layer. Model 2 shares a similar structure but starts with a more substantial first layer of 60 neurons, potentially allowing it to capture more complex representations of the data. Both conclude with a dropout layer and a single-neuron output layer using the sigmoid function, fitting for binary output. The models use Adam optimizer with a learning rate of 0.006 and compile with a binary cross-entropy loss function. They are evaluated using a suite of metrics, including accuracy, precision, recall, and an F1 score calculated by TensorFlow Addons, offering a robust assessment of their predictive performance.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

# Features and target variable

X = df2_resampled[['Pregnancies', 'Glucose', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','SkinThickness', 'Age']]
y = df2_resampled['Outcome']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from tensorflow.keras.layers import Dropout

model1 = Sequential([
    Dense(40, activation='relu', input_shape=(7,)),
    Dropout(0.5),  # Dropout rate of 50%
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model2 = Sequential([
    Dense(60, activation='relu', input_shape=(7,)),
    Dropout(0.5),  # Dropout rate of 50%
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

!pip install tensorflow-addons
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Initialize the optimizer with the specified learning rate
optimizer = Adam(learning_rate=0.006)

# Compile the model with the optimizer and specified metrics, including F1Score from TensorFlow Addons
model1.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy',
                       tf.keras.metrics.Precision(name='precision'),
                       tf.keras.metrics.Recall(name='recall'),
                       tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5, name='f1_score')])
optimizer = Adam(learning_rate=0.006)

model2.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy',
                       tf.keras.metrics.Precision(name='precision'),
                       tf.keras.metrics.Recall(name='recall'),
                       tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5, name='f1_score')])

"""The folowing code implements an exponential decay learning rate schedule for the Adam optimizer, starting at 0.001 and reducing by 4% every 100,000 steps. It's useful for taking larger steps when the training begins and smaller, more precise steps later on. Additionally, it uses a `ReduceLROnPlateau` callback to further reduce the learning rate if there's no improvement in validation loss after ten** epochs, helping to fine-tune the model and avoid getting stuck in local minima. This dynamic adjustment of the learning rate during training is crucial for both speeding up the learning process and improving the model's accuracy."""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Using a more dynamic approach with learning rate schedules
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)

optimizer = Adam(learning_rate=lr_schedule)

# Callback to reduce learning rate when a metric has stopped improving
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)

"""# Results"""

# Historyof model1
history2 = model1.fit(X_train, y_train, epochs=150, validation_split=0.2, batch_size=10, verbose=1)

# Historyof model2
history2 = model2.fit(X_train, y_train, epochs=150, validation_split=0.2, batch_size=10, verbose=1)

# Evaluate the performance of model1 on the test set
loss, accuracy, precision, recall, f1_score = model1.evaluate(X_test, y_test, verbose=0)
# Print out the performance metrics for model1 on the test data
print(f"Test Loss: {loss}")  # Loss value indicates how well or poorly the model performs (lower is better)
print(f"Test Accuracy: {accuracy}")  # Accuracy is the proportion of true results among the total number of cases examined
print(f"Test Precision: {precision}")  # Precision is the ratio of true positive predictions to the total positive predictions
print(f"Test Recall: {recall}")  # Recall (or Sensitivity) is the ratio of true positive predictions to all actual positives
print(f"Test F1 Score: {f1_score}")  # F1 Score is the harmonic mean of Precision and Recall (useful for imbalanced classes)

# Repeat the evaluation for model2
loss, accuracy, precision, recall, f1_score = model2.evaluate(X_test, y_test, verbose=0)
# Print out the performance metrics for model2 on the test data
print(f"Test Loss: {loss}")  # Indicates how well model2 performs, with a lower loss value being preferable
print(f"Test Accuracy: {accuracy}")  # The accuracy of model2, showing the overall correct predictions
print(f"Test Precision: {precision}")  # Precision for model2, indicating the accuracy of positive predictions
print(f"Test Recall: {recall}")  # Recall for model2, showing how many actual positives were correctly identified
print(f"Test F1 Score: {f1_score}")  # The F1 Score for model2, balancing Precision and Recall in one metric

"""The results of evaluating two distinct neural network models on the test set indicate the following performances:
Upon evaluating two neural network models on the test set, we observed distinct outcomes:

Model 1 reported a Test Loss of roughly 0.554, indicative of its moderate performance on the error rate scale. It recorded an Accuracy of approximately 69.5%, suggesting it correctly predicted the diabetes outcome for just under seventy percent of the cases in the test set. The Precision was measured at about 65.38%, denoting a fairly good rate of true positives among all positive predictions. The Recall stood at approximately 84.16%, reflecting the model's ability to identify a large majority of the actual positive cases. The F1 Score, a harmonized measure of Precision and Recall, was around 73.59%, pointing to a decent balance but also hinting at possible improvement areas, especially in the context of imbalanced classes.

Model 2, on the other hand, showed a lower Test Loss of approximately 0.514, suggesting a slight edge in predictive accuracy over Model 1. Its Accuracy climbed to nearly 76.5%, indicating that it could correctly classify a higher percentage of diabetes instances. With a Precision of roughly 74.55%, Model 2 proved to be more precise in its positive predictions. The Recall was about 81.19%, indicating its proficiency in identifying a substantial share of true positive instances. The F1 Score for Model 2 was near 77.73%, surpassing Model 1 and indicating a superior balance of Precision and Recall.

Overall, Model 2 outshined Model 1 in every assessed metric, hinting that the refinements in Model 2's design or tuning were successful in sharpening its predictive prowess for diabetes classification.

# Graphs
"""

import matplotlib.pyplot as plt

def plot_history1(history):
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))

    # Plot training and validation accuracy for model 1
    axs[0, 0].plot(history.history['accuracy'])
    axs[0, 0].plot(history.history['val_accuracy'])
    axs[0, 0].set_title('Model Accuracy')
    axs[0, 0].set_ylabel('Accuracy')
    axs[0, 0].set_xlabel('Epoch')
    axs[0, 0].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation loss for model 1
    axs[0, 1].plot(history.history['loss'])
    axs[0, 1].plot(history.history['val_loss'])
    axs[0, 1].set_title('Model Loss')
    axs[0, 1].set_ylabel('Loss')
    axs[0, 1].set_xlabel('Epoch')
    axs[0, 1].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation precision  for model 1
    axs[1, 0].plot(history.history['precision'])
    axs[1, 0].plot(history.history['val_precision'])
    axs[1, 0].set_title('Model Precision')
    axs[1, 0].set_ylabel('Precision')
    axs[1, 0].set_xlabel('Epoch')
    axs[1, 0].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation recall for model 1
    axs[1, 1].plot(history.history['recall'])
    axs[1, 1].plot(history.history['val_recall'])
    axs[1, 1].set_title('Model Recall')
    axs[1, 1].set_ylabel('Recall')
    axs[1, 1].set_xlabel('Epoch')
    axs[1, 1].legend(['Train', 'Val'], loc='upper left')
    plt.show()

plot_history1(history1)

"""**Description of the plots for model 1**

The training and validation metrics for the neural network model, observed over numerous epochs, display a multifaceted picture of the model's performance. Initially, both the accuracy and the loss metrics improve significantly, with the accuracy increasing and loss decreasing sharply. However, as epochs continue, the accuracy stabilizes, with the training consistently outperforming the validation, a typical indication that the model is fitting well to the training data. The loss metric similarly plateaus, suggesting the model has likely reached its performance limit given the current architecture. Precision and recall, however, exhibit high variability, particularly in the validation set. This fluctuation could indicate the model's struggles with making consistently correct predictions of the positive class across the varied slices of data. The recall metric does not show a pattern of improvement, which implies challenges in identifying all relevant instances. These insights point to a potential for model optimization, possibly through hyperparameter tuning, regularization techniques, or even a re-evaluation of the feature engineering process to enhance the model's predictive robustness and generalization.
"""

def plot_history2(history):
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))

    # Plot training and validation accuracy for model 1
    axs[0, 0].plot(history.history['accuracy'])
    axs[0, 0].plot(history.history['val_accuracy'])
    axs[0, 0].set_title('Model Accuracy')
    axs[0, 0].set_ylabel('Accuracy')
    axs[0, 0].set_xlabel('Epoch')
    axs[0, 0].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation loss for model 1
    axs[0, 1].plot(history.history['loss'])
    axs[0, 1].plot(history.history['val_loss'])
    axs[0, 1].set_title('Model Loss')
    axs[0, 1].set_ylabel('Loss')
    axs[0, 1].set_xlabel('Epoch')
    axs[0, 1].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation precision  for model 1
    axs[1, 0].plot(history.history['precision'])
    axs[1, 0].plot(history.history['val_precision'])
    axs[1, 0].set_title('Model Precision')
    axs[1, 0].set_ylabel('Precision')
    axs[1, 0].set_xlabel('Epoch')
    axs[1, 0].legend(['Train', 'Val'], loc='upper left')

    # Plot training and validation recall for model 1
    axs[1, 1].plot(history.history['recall'])
    axs[1, 1].plot(history.history['val_recall'])
    axs[1, 1].set_title('Model Recall')
    axs[1, 1].set_ylabel('Recall')
    axs[1, 1].set_xlabel('Epoch')
    axs[1, 1].legend(['Train', 'Val'], loc='upper left')
    plt.show()

plot_history2(history2)

"""**Description of the plots for model 2**

The Model Accuracy plot demonstrates that while training accuracy is somewhat stable and higher than validation accuracy, there is notable fluctuation in the validation accuracy which may point to a model that's overfitting or to a dataset with intrinsic variability. In the Model Loss plot, both training and validation loss decrease sharply and early, then level off, indicating a quick learning phase followed by a plateau, with minimal gap between training and validation loss suggesting good generalization. The Model Precision plot reveals a decrease after the initial spike, followed by a period of relative stability with some oscillation, where training and validation lines closely follow each other. Similarly, the Model Recall plot depicts a convergence of training and validation recall after the initial learning period, with both displaying substantial fluctuation throughout the training process. Overall, the plots reflect a model that learns rapidly and achieves a degree of stability, though the variability in precision and recall suggests there might be room for improvement in the model's consistency.

## Discussion and Future Work


In my examination of two neural network models for diabetes prediction, Model 2 demonstrated superior performance compared to Model 1, as evidenced by its lower test loss and higher values across all key metrics—accuracy, precision, recall, and F1 score. The training and validation curves for Model 1 depicted significant oscillations, particularly in recall and precision, which hinted at a model less adept at consistent predictions. In contrast, Model 2's training curves were more stable, indicating a robustness that likely contributed to its enhanced generalization capabilities.

Moving forward, fine-tuning through hyperparameter optimization, further regularization, advanced feature engineering, and exploring ensemble methods could yield even better results.Additonally,I would like to explore more complex models, additional features, and larger datasets. We also discuss the implications of deploying such predictive models in healthcare settings, including ethical considerations like data privacy and the risk of over-reliance on automated diagnoses.
"""